"""
Core message API routes for Claude Code Provider Balancer.
Handles the main /v1/messages endpoint and token counting.
"""

import json
import uuid
from typing import Any, Dict, Optional
from dataclasses import dataclass

from fastapi import APIRouter, Request
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import ValidationError

from handlers.message_handler import MessageHandler
from models import MessagesRequest, TokenCountResponse
from core.provider_manager import ProviderManager, ProviderType
from core.streaming import (
    has_active_broadcaster, handle_duplicate_stream_request,
    create_broadcaster, register_broadcaster, unregister_broadcaster
)
from caching import (
    generate_request_signature, handle_duplicate_request,
    complete_and_cleanup_request
)
from conversion import (
    convert_anthropic_to_openai_messages, convert_anthropic_tools_to_openai,
    convert_anthropic_tool_choice_to_openai, convert_openai_to_anthropic_response
)
from utils import LogRecord, LogEvent, info, warning, error, debug
from core.provider_manager.health import validate_response_health


@dataclass
class RequestContext:
    """Encapsulates all request processing context and data."""
    request_id: str
    request: Request
    raw_body: bytes
    parsed_body: Dict[str, Any]
    clean_request_body: Dict[str, Any]
    messages_request: MessagesRequest
    provider_name: Optional[str]
    signature: str
    original_headers: Dict[str, str]
    
    @property
    def is_streaming(self) -> bool:
        """Check if this is a streaming request."""
        return self.messages_request.stream or False


def create_error_preview(response_content, max_preview_length: int = 200):
    """创建响应体的预览和完整内容"""
    if isinstance(response_content, list):
        # Stream response (List[str])
        full_content = "".join(response_content)
    elif isinstance(response_content, str):
        full_content = response_content
    elif isinstance(response_content, dict):
        full_content = json.dumps(response_content, ensure_ascii=False)
    else:
        full_content = str(response_content)
    
    # 创建预览版本（用于控制台）
    if len(full_content) > max_preview_length:
        preview = full_content[:max_preview_length] + "..."
    else:
        preview = full_content
    
    return preview, full_content


def create_messages_router(provider_manager: ProviderManager, settings: Any) -> APIRouter:
    """Create messages router with dependencies."""
    router = APIRouter(prefix="/v1", tags=["API"])
    message_handler = MessageHandler(provider_manager, settings)

    async def _preprocess_request(request: Request, request_id: str) -> RequestContext:
        """Extract and validate request data, create context object."""
        # Get request body for logging and caching
        raw_body = await request.body()
        parsed_body = json.loads(raw_body.decode('utf-8'))
        
        # Extract provider parameter separately before validation
        provider_name = parsed_body.pop("provider", None)
        
        # Generate request signature for deduplication (without provider)
        signature = generate_request_signature(parsed_body)
        
        # Validate the remaining fields with MessagesRequest
        messages_request = MessagesRequest(**parsed_body)
        
        # Add provider back to parsed_body for logging and other uses
        if provider_name:
            parsed_body["provider"] = provider_name
        
        # Log request received
        info(
            LogRecord(
                event=LogEvent.REQUEST_RECEIVED.value,
                message=message_handler._create_request_summary(parsed_body),
                request_id=request_id,
                data={
                    "model": messages_request.model,
                    "stream": messages_request.stream,
                    "provider": provider_name,
                },
            )
        )
        
        # Create clean request body without balancer-specific fields for provider requests
        clean_request_body = {k: v for k, v in parsed_body.items() if k not in ['provider']}
        original_headers = dict(request.headers)
        
        return RequestContext(
            request_id=request_id,
            request=request,
            raw_body=raw_body,
            parsed_body=parsed_body,
            clean_request_body=clean_request_body,
            messages_request=messages_request,
            provider_name=provider_name,
            signature=signature,
            original_headers=original_headers
        )

    @router.post("/messages", response_model=None, status_code=200)
    async def create_message_proxy(request: Request) -> JSONResponse:
        """Proxy endpoint for Anthropic Messages API."""
        request_id = str(uuid.uuid4())
        
        try:
            # Preprocess request and create context
            context = await _preprocess_request(request, request_id)
            
            # For streaming requests, first check if there's an active broadcaster for this signature
            if context.is_streaming and has_active_broadcaster(context.signature):
                try:
                    # Try to connect to existing broadcaster for duplicate stream request
                    stream_generator = handle_duplicate_stream_request(context.signature, request, request_id)
                    return StreamingResponse(
                        stream_generator,
                        media_type="text/event-stream",
                        headers={"x-provider-used": "broadcaster-duplicate"}
                    )
                except Exception as e:
                    # Fallback if broadcaster connection fails
                    debug(
                        LogRecord(
                            "broadcaster_connection_failed",
                            f"Failed to connect to active broadcaster: {str(e)}",
                            request_id,
                            {
                                "signature": context.signature[:16] + "...",
                                "error": str(e)
                            }
                        )
                    )
            
            # Check for cached duplicate requests
            duplicate_result = await handle_duplicate_request(
                context.signature, request_id, context.is_streaming, context.clean_request_body
            )
            if duplicate_result is not None:
                return duplicate_result
            
            # Select all available provider options for failover
            provider_options = message_handler.select_model_and_provider_options(
                context.messages_request.model, request_id, context.provider_name
            )
            if not provider_options:
                error_msg = message_handler._create_no_providers_error_message(context.messages_request.model, context.provider_name)
                return await message_handler._log_and_return_error_response(
                    request, Exception(error_msg), request_id, 404, context.signature
                )
            
            # Log available options
            info(
                LogRecord(
                    event=LogEvent.REQUEST_START.value,
                    message=f"Processing request with {len(provider_options)} provider option(s)",
                    request_id=request_id,
                    data={
                        "client_model": context.messages_request.model,
                        "available_options": len(provider_options),
                        "primary_provider": provider_options[0][1].name,
                        "stream": context.is_streaming,
                    },
                )
            )
            
            # Try providers in order until one succeeds
            max_attempts = len(provider_options)
            last_exception = None
            
            for attempt in range(max_attempts):
                target_model, current_provider = provider_options[attempt]
                
                try:
                    # Make actual request to current provider
                    if current_provider.type == ProviderType.ANTHROPIC:
                        if context.is_streaming:
                            # For streaming requests, we'll handle the response in the streaming section below
                            # Don't make the request here to avoid duplicate logging
                            response = None
                        else:
                            # For non-streaming requests, use the regular request method
                            response = await message_handler.make_anthropic_request(
                                current_provider, context.clean_request_body, request_id, 
                                False, context.original_headers
                            )
                    elif current_provider.type == ProviderType.OPENAI:
                        # Convert to OpenAI format first
                        openai_messages = convert_anthropic_to_openai_messages(
                            context.messages_request.messages, context.messages_request.system, request_id
                        )
                        openai_tools = convert_anthropic_tools_to_openai(context.messages_request.tools)
                        openai_tool_choice = convert_anthropic_tool_choice_to_openai(
                            context.messages_request.tool_choice, request_id
                        )
                        
                        openai_params = {
                            "model": target_model,
                            "messages": openai_messages,
                            "max_tokens": context.messages_request.max_tokens,
                            "stream": context.is_streaming,
                        }
                        
                        if context.messages_request.temperature is not None:
                            openai_params["temperature"] = context.messages_request.temperature
                        if context.messages_request.top_p is not None:
                            openai_params["top_p"] = context.messages_request.top_p
                        if context.messages_request.stop_sequences:
                            openai_params["stop"] = context.messages_request.stop_sequences
                        if openai_tools:
                            openai_params["tools"] = openai_tools
                        if openai_tool_choice:
                            openai_params["tool_choice"] = openai_tool_choice
                        
                        response = await message_handler.make_openai_request(
                            current_provider, openai_params, request_id,
                            context.is_streaming, context.original_headers
                        )
                    else:
                        raise ValueError(f"Unsupported provider type: {current_provider.type}")
                    
                    # Handle streaming response
                    if context.is_streaming:
                        # Add provider information to headers for streaming responses
                        stream_headers = {"x-provider-used": current_provider.name}
                        
                        if current_provider.type == ProviderType.ANTHROPIC:
                            # For Anthropic streaming requests, we handle them directly here
                            collected_chunks = []
                            
                            async def stream_anthropic_response():
                                """Simplified Anthropic streaming using parallel broadcaster"""
                                broadcaster = None
                                try:
                                    # Create parallel broadcaster for handling multiple clients
                                    broadcaster = create_broadcaster(request, request_id, current_provider.name)
                                    
                                    # Register broadcaster for duplicate request handling
                                    register_broadcaster(context.signature, broadcaster)
                                    
                                    # Create provider stream from response using real-time streaming
                                    async def provider_stream():
                                        try:
                                            # Use the new streaming method that maintains httpx context
                                            async for response_obj in message_handler.make_anthropic_streaming_request(
                                                current_provider, context.clean_request_body, request_id, context.original_headers
                                            ):
                                                async for chunk in response_obj.aiter_text():
                                                    collected_chunks.append(chunk)
                                                    yield chunk
                                        except Exception:
                                            # Error will be logged by ParallelBroadcaster.stream_from_provider()
                                            raise
                                    
                                    # Use broadcaster to handle parallel streaming with disconnect detection
                                    async for chunk in broadcaster.stream_from_provider(provider_stream()):
                                        yield chunk
                                        
                                except Exception as e:
                                    error(
                                        LogRecord(
                                            "stream_anthropic_error",
                                            f"Error in Anthropic streaming: {type(e).__name__}: {e}",
                                            request_id,
                                            {
                                                "provider": current_provider.name,
                                                "error": str(e)
                                            }
                                        )
                                    )
                                    raise
                                finally:
                                    # Unregister broadcaster when streaming completes
                                    if broadcaster:
                                        unregister_broadcaster(signature)
                            
                            return StreamingResponse(
                                stream_anthropic_response(),
                                media_type="text/event-stream",
                                headers=stream_headers
                            )
                        else:
                            # For OpenAI providers - handle streaming response
                            if hasattr(response, '__aiter__'):
                                # Response is an AsyncStream object, collect chunks for caching while streaming
                                collected_chunks = []
                                
                                async def stream_openai_response():
                                    """Handle OpenAI streaming response and convert to Anthropic format"""
                                    broadcaster = None
                                    try:
                                        # Create parallel broadcaster for handling multiple clients
                                        broadcaster = create_broadcaster(request, request_id, current_provider.name)
                                        
                                        # Register broadcaster for duplicate request handling
                                        register_broadcaster(signature, broadcaster)
                                        
                                        # Create provider stream from OpenAI AsyncStream
                                        async def provider_stream():
                                            try:
                                                async for chunk in response:
                                                    # Convert OpenAI chunk to Anthropic SSE format
                                                    if hasattr(chunk, 'choices') and chunk.choices:
                                                        choice = chunk.choices[0]
                                                        if hasattr(choice, 'delta') and choice.delta:
                                                            delta = choice.delta
                                                            if hasattr(delta, 'content') and delta.content:
                                                                # Create Anthropic-style text delta event
                                                                anthropic_chunk = {
                                                                    "type": "content_block_delta",
                                                                    "index": 0,
                                                                    "delta": {
                                                                        "type": "text_delta",
                                                                        "text": delta.content
                                                                    }
                                                                }
                                                                sse_data = f"data: {json.dumps(anthropic_chunk)}\n\n"
                                                                collected_chunks.append(sse_data)
                                                                yield sse_data
                                                            elif hasattr(choice, 'finish_reason') and choice.finish_reason:
                                                                # Create Anthropic-style message stop event
                                                                stop_chunk = {
                                                                    "type": "message_stop"
                                                                }
                                                                sse_data = f"data: {json.dumps(stop_chunk)}\n\n"
                                                                collected_chunks.append(sse_data)
                                                                yield sse_data
                                            except Exception as e:
                                                error(
                                                    LogRecord(
                                                        "openai_stream_error",
                                                        f"Error in OpenAI stream: {type(e).__name__}: {e}",
                                                        request_id,
                                                        {
                                                            "provider": current_provider.name,
                                                            "error": str(e)
                                                        }
                                                    )
                                                )
                                                raise
                                        
                                        # Use broadcaster to handle parallel streaming with disconnect detection
                                        async for chunk in broadcaster.stream_from_provider(provider_stream()):
                                            yield chunk
                                            
                                    except Exception as e:
                                        error(
                                            LogRecord(
                                                "stream_openai_error",
                                                f"Error in OpenAI streaming: {type(e).__name__}: {e}",
                                                request_id,
                                                {
                                                    "provider": current_provider.name,
                                                    "error": str(e)
                                                }
                                            )
                                        )
                                        raise
                                    finally:
                                        # Unregister broadcaster when streaming completes
                                        if broadcaster:
                                            unregister_broadcaster(signature)
                                        
                                        # Complete the request with collected chunks
                                        complete_and_cleanup_request(signature, collected_chunks, collected_chunks, True, current_provider.name)
                                        
                                        # Mark provider success for sticky routing and failure count reset
                                        current_provider.mark_success()
                                        if provider_manager:
                                            provider_manager.mark_provider_success(current_provider.name)
                                            # 记录成功的健康检查结果
                                            provider_manager.record_health_check_result(
                                                current_provider.name, False, None, request_id
                                            )
                                        
                                        # Log request completion
                                        info(
                                            LogRecord(
                                                event=LogEvent.REQUEST_COMPLETED.value,
                                                message=f"OpenAI streaming request completed: {current_provider.name}",
                                                request_id=request_id,
                                                data={
                                                    "provider": current_provider.name,
                                                    "model": target_model,
                                                    "stream": True,
                                                    "provider_type": "openai",
                                                    "chunks_count": len(collected_chunks),
                                                    "attempt": attempt + 1,
                                                }
                                            )
                                        )
                                
                                return StreamingResponse(
                                    stream_openai_response(),
                                    media_type="text/event-stream",
                                    headers=stream_headers
                                )
                            else:
                                # If response is not streamable, convert to streaming format
                                response_data = response.json() if hasattr(response, 'json') else response
                                collected_chunks = [f"data: {json.dumps(response_data)}\n\n"]
                                
                                async def convert_to_stream():
                                    yield f"data: {json.dumps(response_data)}\n\n"
                                
                                complete_and_cleanup_request(signature, response_data, collected_chunks, True, current_provider.name)
                                
                                return StreamingResponse(
                                    convert_to_stream(),
                                    media_type="text/event-stream",
                                    headers=stream_headers
                                )
                    
                    # Handle non-streaming response
                    if current_provider.type == ProviderType.OPENAI:
                        # Convert OpenAI response back to Anthropic format
                        anthropic_response = convert_openai_to_anthropic_response(response, request_id)
                        # Convert Pydantic model to dict for JSON serialization
                        response_content = anthropic_response.model_dump()
                    else:
                        # For Anthropic providers, handle httpx.Response objects
                        if hasattr(response, 'json'):
                            response_content = response.json()
                        elif isinstance(response, dict):
                            response_content = response
                        else:
                            # Convert non-serializable response objects to dict
                            response_content = {"error": {"type": "serialization_error", "message": "Unable to serialize response"}}
                    
                    # Check provider health based on response content
                    if provider_manager:
                        # Get HTTP status code from response
                        response_status_code = None
                        if hasattr(response, 'status_code'):
                            response_status_code = response.status_code
                        
                        # 使用简化的健康检查
                        is_error_detected, error_reason = validate_response_health(
                            response_content,
                            response_status_code,
                            provider_manager.settings.get('unhealthy_http_codes', []),
                            provider_manager.settings.get('unhealthy_response_body_patterns', [])
                        )
                        
                        # 如果检测到错误，记录 PROVIDER_REQUEST_ERROR 日志
                        if is_error_detected:
                            preview, full_content = create_error_preview(response_content)
                            error(
                                LogRecord(
                                    LogEvent.PROVIDER_REQUEST_ERROR.value,
                                    f"Provider {current_provider.name} non-streaming request failed: {error_reason} - Response preview: {preview}",
                                    request_id,
                                    {
                                        "provider": current_provider.name,
                                        "error_reason": error_reason,
                                        "response_preview": preview,
                                        "full_response_body": full_content,
                                        "request_type": "non-streaming",
                                        "http_status_code": response_status_code
                                    }
                                )
                            )
                        
                        # 使用ProviderManager的错误计数机制
                        should_mark_unhealthy = provider_manager.record_health_check_result(
                            current_provider.name, is_error_detected, error_reason, request_id
                        )
                        
                        if should_mark_unhealthy:
                            
                            warning(
                                LogRecord(
                                    "provider_unhealthy_non_stream",
                                    f"Provider {current_provider.name} marked unhealthy after reaching error threshold, attempting failover",
                                    request_id,
                                    {
                                        "provider": current_provider.name,
                                        "error_reason": error_reason,
                                        "attempt": attempt + 1,
                                        "remaining_attempts": max_attempts - attempt - 1,
                                        "can_failover": True
                                    }
                                )
                            )
                            
                            # Create a health validation exception to trigger failover
                            health_validation_error = Exception(f"Provider health validation failed: {error_reason}")
                            
                            # If we have more providers to try, continue to next iteration
                            if attempt < max_attempts - 1:
                                last_exception = health_validation_error
                                continue
                            else:
                                # This was the last provider, return the health validation error
                                return await message_handler._log_and_return_error_response(request, health_validation_error, request_id, 500, signature)
                    
                    # Provider is healthy, complete the request and cleanup
                    # For non-streaming responses, cache the actual content dict instead of JSON string array
                    
                    # uncomment this if you want to debug the response content before returning
                    # # Debug: Log response content before return
                    # debug(
                    #     LogRecord(
                    #         event="response_content",
                    #         message=f"Response content before return: type={type(response_content)}, content={str(response_content)[:500]}...",
                    #         request_id=request_id,
                    #         data={
                    #             "provider": current_provider.name,
                    #             "response_type": type(response_content).__name__,
                    #             "content_length": len(str(response_content)) if response_content else 0
                    #         }
                    #     )
                    # )
                    
                    complete_and_cleanup_request(signature, response_content, response_content, False, current_provider.name)
                    
                    # Mark provider success for sticky routing and failure count reset
                    current_provider.mark_success()
                    if provider_manager:
                        provider_manager.mark_provider_success(current_provider.name)
                        # 记录成功的健康检查结果
                        provider_manager.record_health_check_result(
                            current_provider.name, False, None, request_id
                        )
                    
                    info(
                        LogRecord(
                            event=LogEvent.REQUEST_COMPLETED.value,
                            message=f"Request completed: {current_provider.name}",
                            request_id=request_id,
                            data={
                                "provider": current_provider.name,
                                "model": target_model,
                                "tokens": response_content.get("usage", {}),
                                "attempt": attempt + 1,
                            },
                        )
                    )
                    
                    # Create response with provider information in headers  
                    response_headers = {"x-provider-used": current_provider.name}
                    return JSONResponse(content=response_content, headers=response_headers)
                    
                except Exception as e:
                    last_exception = e
                    
                    # Get HTTP status code if available
                    http_status_code = getattr(e, 'status_code', None) or (
                        getattr(e, 'response', None) and getattr(e.response, 'status_code', None)
                    )
                    
                    # Special handling for 401 Unauthorized and 403 Forbidden with Claude Code Official
                    if http_status_code in [401, 403] and current_provider.name == "Claude Code Official":
                        # Handle OAuth authorization required
                        if provider_manager:
                            login_url = provider_manager.handle_oauth_authorization_required(current_provider, http_status_code)
                            if login_url:
                                # For OAuth authorization flow, don't failover and return the auth error directly
                                # The user needs to complete the OAuth flow
                                return await message_handler._log_and_return_error_response(request, e, request_id, http_status_code, signature)
                    
                    # Use provider_manager to determine error handling strategy
                    if provider_manager:
                        error_reason, should_mark_unhealthy, can_failover = provider_manager.get_error_handling_decision(
                            e, http_status_code, messages_request.stream
                        )
                    else:
                        # Default behavior if provider_manager is not available
                        should_mark_unhealthy = True
                        can_failover = True
                        error_reason = "unknown_error"
                    
                    # Mark current provider as failed if unhealthy threshold is reached
                    provider_marked_unhealthy = False
                    if provider_manager:
                        # Use error counting mechanism before marking as failed
                        provider_marked_unhealthy = provider_manager.record_health_check_result(
                            current_provider.name, True, error_reason, request_id
                        )
                        # Only mark as failed if threshold is reached
                        if provider_marked_unhealthy:
                            current_provider.mark_failure()
                        # If threshold not reached, don't mark as failed - just record the error
                    else:
                        # Fallback: use original logic if no provider manager
                        current_provider.mark_failure()
                        provider_marked_unhealthy = True
                    
                    # Only attempt failover if provider was marked as unhealthy
                    if not provider_marked_unhealthy:
                        # Provider not marked unhealthy, return error immediately (no failover needed)
                        if provider_manager:
                            provider_manager.mark_provider_used(current_provider.name)
                        
                        info(
                            LogRecord(
                                event=LogEvent.PROVIDER_ERROR_BELOW_THRESHOLD.value,
                                message="Provider not marked unhealthy (error count below threshold), returning error to client",
                                request_id=request_id,
                                data={
                                    "provider": current_provider.name,
                                    "error_reason": error_reason,
                                    "http_status_code": http_status_code,
                                    "provider_marked_unhealthy": provider_marked_unhealthy
                                }
                            )
                        )
                        # 从异常中提取HTTP状态码，保持原始错误状态码
                        http_status_code_from_exc = getattr(e, 'status_code', None) or (
                            getattr(e, 'response', None) and getattr(e.response, 'status_code', None)
                        )
                        status_code = http_status_code_from_exc if http_status_code_from_exc else 500
                        return await message_handler._log_and_return_error_response(request, e, request_id, status_code, signature)
                    
                    # Provider was marked unhealthy, now check if we can failover
                    if not can_failover:
                        # Provider is unhealthy but we cannot failover (e.g., streaming response headers already sent)
                        info(
                            LogRecord(
                                event=LogEvent.PROVIDER_UNHEALTHY_NO_FAILOVER.value,
                                message="Provider marked unhealthy but cannot failover for this request type, returning error to client",
                                request_id=request_id,
                                data={
                                    "provider": current_provider.name,
                                    "error_reason": error_reason,
                                    "can_failover": can_failover,
                                    "is_streaming": messages_request.stream,
                                    "provider_marked_unhealthy": provider_marked_unhealthy
                                }
                            )
                        )
                        # 从异常中提取HTTP状态码，保持原始错误状态码
                        http_status_code_from_exc = getattr(e, 'status_code', None) or (
                            getattr(e, 'response', None) and getattr(e.response, 'status_code', None)
                        )
                        status_code = http_status_code_from_exc if http_status_code_from_exc else 500
                        return await message_handler._log_and_return_error_response(request, e, request_id, status_code, signature)
                    
                    # If we have more providers to try, continue to next iteration
                    if attempt < max_attempts - 1:
                        next_target_model, next_provider = provider_options[attempt + 1]
                        info(
                            LogRecord(
                                event=LogEvent.PROVIDER_FALLBACK.value,
                                message=f"Falling back to provider: {next_provider.name} with model: {next_target_model}",
                                request_id=request_id,
                                data={
                                    "failed_provider": current_provider.name,
                                    "failed_model": target_model,
                                    "fallback_provider": next_provider.name,
                                    "fallback_model": next_target_model,
                                    "attempt": attempt + 2,
                                    "total_attempts": max_attempts
                                }
                            )
                        )
                    # If this is the last attempt, the loop will end and we'll return the error
            
            # All providers failed, return the last exception
            error(
                LogRecord(
                    event=LogEvent.ALL_PROVIDERS_FAILED.value,
                    message=f"All {max_attempts} provider(s) failed for model: {messages_request.model}",
                    request_id=request_id,
                    data={
                        "model": messages_request.model,
                        "total_attempts": max_attempts,
                        "providers_tried": [opt[1].name for opt in provider_options]
                    }
                ),
                exc=last_exception
            )
            
            # Create a generic error message for the client that doesn't expose provider details
            client_error_message = message_handler._create_no_providers_error_message(messages_request.model)
            client_error = Exception(client_error_message)
            
            # 当所有providers都不可用时，这是balancer自己的错误，应该返回503 Service Unavailable
            return await message_handler._log_and_return_error_response(request, client_error, request_id, 503, signature)
                
        except ValidationError as e:
            # ValidationError发生在生成signature之前，无需cleanup
            return await message_handler._log_and_return_error_response(request, e, request_id, 400)
        except json.JSONDecodeError as e:
            # JSONDecodeError发生在生成signature之前，无需cleanup
            return await message_handler._log_and_return_error_response(request, e, request_id, 400)
        except Exception as e:
            # 通用异常可能在任何阶段发生，尝试cleanup（如果signature不存在会被安全忽略）
            # 尝试从异常中提取HTTP状态码，如果没有则默认为500
            http_status_code = getattr(e, 'status_code', None) or (
                getattr(e, 'response', None) and getattr(e.response, 'status_code', None)
            )
            status_code = http_status_code if http_status_code else 500
            return await message_handler._log_and_return_error_response(request, e, request_id, status_code, locals().get('signature'))

    @router.post("/messages/count_tokens", response_model=TokenCountResponse)
    async def count_tokens_endpoint(request: Request) -> TokenCountResponse:
        """Estimates token count for given Anthropic messages and system prompt."""
        return await message_handler.count_tokens(request)

    return router